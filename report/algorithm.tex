\mypar{Connected components}
%Precisely define sorting problem you consider.
%\mypar{Sorting algorithms}
%Explain the algorithm you use including their costs.
%
%As an aside, don't talk about "the complexity of the algorithm.'' It's incorrect,
%problems have a complexity, not algorithms.

For an undirected graph $G=(V,E)$, the connected components are the ensemble
of connected subgraphs, where connected means that for any two vertices there exists a path along
the edges connecting them.
A straightforward algorithm to find the connected components is to perform either
a breath or depth first search starting from a random vertex in $V$,
and give the same label to all vertices reached. The search is then repeated, starting from an
unlabeled vertex.
This has a cost in terms of memory accesses of $\Theta(\abs{E} + \abs{V})$, which turns out
to be optimal \cite{Hopcroft}.

\section{Proposed Algorithm}\label{sec:yourmethod}
%Now comes the ``beef'' of the report, where you explain what you
%did. Again, organize it in paragraphs with titles. As in every section
%you start with a very brief overview of the section.
%
%In this section, structure is very important so one can follow the technical content.
%
%Mention and cite any external resources that you used including libraries or other code.


Unfortunately this algorithm does not parallelize in a straightforward
way. Instead we first implemented
an algorithm proposed by Uzi Vishkin \cite{PCompArticle} and later described in a class by Pavel
Tvrdik \cite{PCompClass}. This algorithm casts the problem in terms of the generation of a
forest, where the vertices of the same connected component belong to the same tree, and its root
can be used as the representative.\\
We define a star as a tree of height one, a singleton as a tree with a single element, and
use the variables $n=\abs{V}$ and $m=\abs{E}$.
The algorithm can be summarized as:

%TODO check
\begin{algorithm}[H]
    \caption{Pavel Tvrdik's Connected components}
    \label{algorithm:cc1}
    \begin{algorithmic}[1]
        \Procedure{Hook}{$i, j$}
          \State $p[p[i]] = p[j]$
        \EndProcedure
        \Procedure{connectedComponents}{$n, \text{edges}$}
          \State $p[i] = i \quad \forall i \in \{1,\cdots, n\}$. \Comment{Initialize a list of
parents.}
        \While{Elements of $p$ are changed.}
        \For{$\left<i, j\right> \in \text{edges}$} \Comment{Execute in parallel.}
          \State  \kif $i\ge j$ \kthen Hook($i, j$)
          \State  \kif $\text{isSingleton}(i)$ \kthen Hook($i, j$)
        \EndFor
        \For{$\left<i, j\right> \in \text{edges}$} \Comment{Execute in parallel.}
          \State  \kif isStar(i) \kand $i \neq j$ \kthen Hook($i, j$)
        \EndFor
        \State $p[i] = \text{root}(i) \quad \forall i \in \{1,\cdots, n\}$ \Comment{Compress the
forest in parallel.}
        \EndWhile
        \EndProcedure
   \end{algorithmic}
\end{algorithm}
We defer to \cite{PCompClass} for a proof of correctness.

After implementing this algorithm we found advantageous to remove the constraint that only
singletons and stars can be hooked to another vertex. This allows the algorithm to terminate after
a single pass through the edge list. Extra care is then required during parallel execution: as each
vertex
only has one outgoing connection, we need to avoid the scenario where a thread overwrites a
connection that has been formed by another one.
Therefore whenever we need to generate a hook from $v1$ to $v2$ we choose the following rules:

\begin{enumerate}
    \item $\text{index}(v1) > \text{index}(v2)$.
    \item $v1$ must be a root.
\end{enumerate}

An intuitive proof of correctness follows: rule $1$ means that the graph generated
by the hooks is a directed graph without cycles and
 each vertex has at most one outgoing connection. Therefore, it
must be a forest.
Rule $2$ guarantees that a connection cannot be broken by a
different edge.
After the algorithm terminates all vertices in a tree belong to the same connected
component. The connected component can be canonically represented by the tree's root. % NOTE: sure about canonical?

To implement rule $2$ in a multi-threaded environment we use an atomic
compare and swap.
We compare the parent of the hook's origin with its id. If they
match it means the vertex is a root, and this status was not modified by another thread.
For correctness it does not matter if the % Not the same as \label{algorithm:step:skip_connection}.
destination is a root, but doing so minimizes the tree height. % MAYBE: note that it is root according to thread's cache
Empirically we found that, using the standard library, weak atomics offer better performance compared to
strong atomics.

In pseudocode our algorithm is:

\begin{algorithm}[H]
    \caption{Single pass connected component.}
    \label{algorithm:cc2}
    \begin{algorithmic}[1]
        \Procedure{connectedComponents}{$n, \text{edges}$}
        \State $p[i] = i \quad \forall i \in \{1,\cdots, n\}$.
        \For{$\left<i, j\right> \in \text{edges}$} \Comment{Execute in parallel.}
\label{algorithm:step:loop}
        \While{hook is not successful.}
                \State from = max(root(i), root(j))
                \State to = min(root(i), root(j))
                \State  atomicHook(from, to)
        \EndWhile
        \State  \kif !isRoot(i) \kthen p[i] = root(i) \label{algorithm:step:skip_connection}
        \State  \kif !isRoot(j) \kthen p[j] = root(j)
        \EndFor
        \State $p[i] = \text{root}(i) \quad \forall i \in \{1,\cdots, n\}$ \Comment{Compress the
forest in parallel.} \label{algorithm:step:compression}
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

While step \ref{algorithm:step:skip_connection} is not necessary for correctness, we found that
reusing the already computed vertex's representative leads to a smaller tree height.
We avoided using atomic directives for this step and
the parallel compression \ref{algorithm:step:compression}
therefore our implementations works only on architectures such as x86, where writes to 32 or 64-bits variables, used to store a
vertex's id, are atomic.

We tried implementing the parallel execution of loop \ref{algorithm:step:loop} with  Boost fibers
\cite{Boost}
whose execution is scheduled with a work stealing algorithm, and OpenMP with a dynamic scheduler.
OpenMP performed better by a large margin and thus was used to acquire the
data presented in Section \ref{sec:exp}.

The overall cost of the algorithm is $\Theta((n +
m)\langle H \rangle)$, where $\langle H \rangle$
is the average tree height. Therefore $\langle H \rangle = \Theta(1)$ for a sub-critical random graph,
and on average (over the execution order of the loop) $\langle H
\rangle = \Theta(\log(n))$
for a supercritical random graph \cite{RandomGraph}.

\mypar{Multiple compute nodes}
The algorithm \label{algorithm:cc2} works only on a single compute node with a shared memory model.
Moreover, it is efficient
only when the graph is sparse enough that the
probability of a collision between two processors
trying to update the same parent is low.
%TODO reference the data.

We propose to extend our algorithm by distributing the list of edges evenly among the MPI
processes. %Rank is the label.
Then each one of them computes a forest using only the subset of edges it
received. This local
computation
is followed by a reduction step, where pairs of MPI ranks combine their respective
forests.
The resulting forest is then compressed before the following reduction step.

Using $p$ processes, the total execution time of this extension scales as $\Theta(\frac{(m +
n)}{p}\, \langle H \rangle + n\,\log p)$.

%This sentence is way too long and confusing!
On top of allowing to scale past a single compute node, this approach offers better results on
dense graphs, where the relative cost of the reduction is small, and multiple threads would have
a high probability of collision during the atomic update.
Therefore a different mixture of MPI ranks and OpenMP threads per rank is advised depending on the
density of the graph.
%TODO: maybe insert ref to data.

% This part is not relevant without results
\mypar{Distributed vertices}
\label{section:distributed}
While the described approach works on generic graphs, it performs poorly on very sparse graphs using
a large number of compute nodes. Moreover, the full set of vertex indices must fit in memory, limiting
the graph size to $8$ billion vertices. If the connectivity of a graph the size of a human brain needs
to be studied,
we propose do distribute the representation of the vertices as well.

Often, real world graphs are embedded on a space with some metric, and connections
occur much more frequently between
vertices that are close together. For example the pixel representing features of an
image, or the roads connecting cities
with a known geographical position.

We represent this type of graph with a very simple model: a two dimensional lattice with random
connections between nearest neighbors
only. We split the lattice in as many square tiles as there are processes. Then each process
applies algorithm \ref{algorithm:cc2} to the subset
of edges connecting two vertices in its tile. Finally we process
edges crossing the boundary between different tiles using MPI one sided communication. The list
of the local vertices' parents is stored in an MPI window, so that
the representative of a remote vertex
can be obtained with \verb|MPI_Get|, while a hook can be created with \verb|MPI_Compare_and_swap|.
Therefore only two global synchronization points are necessary: after all edges have been
processed, and after the final compression
of the forest.





